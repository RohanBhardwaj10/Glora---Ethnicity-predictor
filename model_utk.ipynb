{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa839ca-c79a-41c6-a003-817ca6016a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5c6688-3247-4bdd-939b-28d9bd7e1736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 39_1_20170116174525125.jpg.chip.jpg: Invalid ethnicity value 20170116174525125\n",
      "Skipping 61_1_20170109142408075.jpg.chip.jpg: Invalid ethnicity value 20170109142408075\n",
      "Skipping 61_1_20170109150557335.jpg.chip.jpg: Invalid ethnicity value 20170109150557335\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\bhard\\ROHAN [ML & DS]\\DL Practice Works\\Glora_ face ethnicity\\UTKFace\"\n",
    "img_size = (48, 48)\n",
    "ethnicities = 5\n",
    "\n",
    "# Initialize lists\n",
    "images = []\n",
    "ethnicity_labels = []\n",
    "\n",
    "# Loop through images\n",
    "for file in os.listdir(dataset_path):\n",
    "    try:\n",
    "        file_name = file.split(\".\")[0]\n",
    "        parts = file_name.split(\"_\")\n",
    "\n",
    "        ethnicity = int(parts[2])\n",
    "\n",
    "        if ethnicity < 0 or ethnicity >= ethnicities:\n",
    "            print(f\"Skipping {file}: Invalid ethnicity value {ethnicity}\")\n",
    "            continue\n",
    "        img_path = os.path.join(dataset_path, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Append to lists\n",
    "        images.append(img)\n",
    "        ethnicity_labels.append(ethnicity)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {file} due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df91b94d-cab8-46ef-8273-63e02a5bc697",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images).reshape(-1, 48, 48, 3)  \n",
    "ethnicity_labels = to_categorical(ethnicity_labels, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a04be4-2419-4d73-9321-2d4f43721940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: 23705\n",
      "ethnicity_labels shape: 23705\n"
     ]
    }
   ],
   "source": [
    "print(f\"images shape: {len(images)}\")\n",
    "print(f\"ethnicity_labels shape: {len(ethnicity_labels)}\")\n",
    "\n",
    "images = np.array(images[:23705])\n",
    "ethnicity_labels = np.array(ethnicity_labels[:23705])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddae5a08-a86f-4c49-8a6e-e632984ced39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethnicity 0: 10078 samples\n",
      "Ethnicity 1: 4526 samples\n",
      "Ethnicity 2: 3434 samples\n",
      "Ethnicity 3: 3975 samples\n",
      "Ethnicity 4: 1692 samples\n"
     ]
    }
   ],
   "source": [
    "ethnicity_labels = np.argmax(ethnicity_labels, axis=1)\n",
    "unique_classes, counts = np.unique(ethnicity_labels, return_counts=True)\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Ethnicity {cls}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d933b6b-7db2-434b-8c0c-fa8d1ff91cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, ethnicity_labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319e8e3c-675c-4f34-a91f-42cc1b44f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.47043064100019844, 1: 1.0475033141847105, 2: 1.3806057076295866, 3: 1.1927044025157232, 4: 2.8020094562647753}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "ethnicity_classes = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "ethnicity_counts = np.array([10078, 4526, 3434, 3975, 1692])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=ethnicity_classes, y=np.repeat(ethnicity_classes, ethnicity_counts))\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9548511b-8889-43a2-9d2f-527ce25ddbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 46, 46, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 46, 46, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 23, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 21, 21, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " ethnicity_output (Dense)    (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 619,973\n",
      "Trainable params: 619,525\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Input\n",
    "input_layer = Input(shape=(48, 48, 3))\n",
    "\n",
    "# CNN Layers\n",
    "x = Conv2D(32, (3, 3), activation='relu', kernel_regularizer='l2')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu', kernel_regularizer='l2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation='relu', kernel_regularizer='l2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output for ethnicity\n",
    "ethnicity_output = Dense(5, activation='softmax', name='ethnicity_output')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=input_layer, outputs=ethnicity_output)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783d846d-808c-4bc5-aae9-cfbc6749de94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e8dd8b6-0258-4cf8-be89-6da66f1886b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "593/593 [==============================] - 274s 458ms/step - loss: 2.2703 - accuracy: 0.4997 - val_loss: 1.6109 - val_accuracy: 0.5957\n",
      "Epoch 2/20\n",
      "593/593 [==============================] - 266s 448ms/step - loss: 1.4060 - accuracy: 0.6170 - val_loss: 1.9266 - val_accuracy: 0.4189\n",
      "Epoch 3/20\n",
      "593/593 [==============================] - 332s 559ms/step - loss: 1.2476 - accuracy: 0.6449 - val_loss: 1.0981 - val_accuracy: 0.7106\n",
      "Epoch 4/20\n",
      "593/593 [==============================] - 242s 408ms/step - loss: 1.1879 - accuracy: 0.6529 - val_loss: 1.4841 - val_accuracy: 0.4796\n",
      "Epoch 5/20\n",
      "593/593 [==============================] - 1387s 2s/step - loss: 1.1683 - accuracy: 0.6635 - val_loss: 1.2690 - val_accuracy: 0.6271\n",
      "Epoch 6/20\n",
      "593/593 [==============================] - 347s 585ms/step - loss: 1.1481 - accuracy: 0.6706 - val_loss: 1.0345 - val_accuracy: 0.7074\n",
      "Epoch 7/20\n",
      "593/593 [==============================] - 311s 524ms/step - loss: 1.1149 - accuracy: 0.6775 - val_loss: 1.2590 - val_accuracy: 0.6353\n",
      "Epoch 8/20\n",
      "593/593 [==============================] - 373s 628ms/step - loss: 1.1110 - accuracy: 0.6823 - val_loss: 1.2381 - val_accuracy: 0.6199\n",
      "Epoch 9/20\n",
      "593/593 [==============================] - 316s 534ms/step - loss: 1.0923 - accuracy: 0.6881 - val_loss: 1.1450 - val_accuracy: 0.6545\n",
      "Epoch 10/20\n",
      "593/593 [==============================] - 233s 392ms/step - loss: 1.0652 - accuracy: 0.6926 - val_loss: 1.2354 - val_accuracy: 0.6092\n",
      "Epoch 11/20\n",
      "593/593 [==============================] - 252s 425ms/step - loss: 1.0482 - accuracy: 0.6976 - val_loss: 1.0694 - val_accuracy: 0.6785\n",
      "Epoch 12/20\n",
      "593/593 [==============================] - 601s 1s/step - loss: 1.0242 - accuracy: 0.7053 - val_loss: 1.0183 - val_accuracy: 0.7155\n",
      "Epoch 13/20\n",
      "593/593 [==============================] - 225s 379ms/step - loss: 1.0144 - accuracy: 0.7087 - val_loss: 1.3430 - val_accuracy: 0.5467\n",
      "Epoch 14/20\n",
      "593/593 [==============================] - 282s 476ms/step - loss: 0.9908 - accuracy: 0.7116 - val_loss: 1.3685 - val_accuracy: 0.5752\n",
      "Epoch 15/20\n",
      "593/593 [==============================] - 217s 365ms/step - loss: 0.9791 - accuracy: 0.7168 - val_loss: 0.9896 - val_accuracy: 0.7212\n",
      "Epoch 16/20\n",
      "593/593 [==============================] - 315s 532ms/step - loss: 0.9408 - accuracy: 0.7270 - val_loss: 0.9721 - val_accuracy: 0.7243\n",
      "Epoch 17/20\n",
      "593/593 [==============================] - 228s 385ms/step - loss: 0.9342 - accuracy: 0.7271 - val_loss: 1.3103 - val_accuracy: 0.5695\n",
      "Epoch 18/20\n",
      "593/593 [==============================] - 207s 350ms/step - loss: 0.9211 - accuracy: 0.7273 - val_loss: 1.0188 - val_accuracy: 0.6973\n",
      "Epoch 19/20\n",
      "593/593 [==============================] - 955s 2s/step - loss: 0.8976 - accuracy: 0.7396 - val_loss: 0.9310 - val_accuracy: 0.7488\n",
      "Epoch 20/20\n",
      "593/593 [==============================] - 211s 355ms/step - loss: 0.8864 - accuracy: 0.7378 - val_loss: 0.9713 - val_accuracy: 0.7300\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_onehot = to_categorical(y_train, num_classes=5)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=5)\n",
    "\n",
    "sample_weights_ethnicity = np.array([class_weight_dict[label] for label in y_train])\n",
    "\n",
    "fitting = model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot, \n",
    "    validation_data=(X_test, y_test_onehot),  \n",
    "    sample_weight=sample_weights_ethnicity,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a0b6e3d-06e2-4579-af30-ca5a55c3248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ethnicity_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358a5c1-1a35-4b60-9120-0153d41af395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
